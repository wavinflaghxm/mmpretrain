{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmengine.dataset import DefaultSampler, pseudo_collate\n",
    "from mmdet.datasets.objects365 import Objects365V2Dataset\n",
    "from mmpretrain.datasets.transforms import *\n",
    "from mmpretrain.models import ClsDataPreprocessor\n",
    "\n",
    "from mmengine import Config\n",
    "from mmpretrain.models import build_classifier\n",
    "\n",
    "from projects.ma_clip.datasets import InstanceDataset, LoadInstanceImage\n",
    "from projects.ma_clip.models import *\n",
    "from projects.clip.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/hxm/ovd/mmpretrain/projects/clip/models/clip.py:90: UserWarning: The value of output_dims should be 512, but it is currently set to 256. Please update it in your config file.\n",
      "  f'The value of {key} should be {value}, but it is currently '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/25 19:35:55 - mmengine - INFO - load model from: ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth\n",
      "05/25 19:35:55 - mmengine - INFO - Loads checkpoint by local backend from path: ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth\n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual_projection - torch.Size([768, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.positional_embedding - torch.Size([77, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.text_projection - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.logit_scale - torch.Size([]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.cls_token - torch.Size([1, 1, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.pos_embed - torch.Size([1, 197, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.patch_embed.projection.weight - torch.Size([768, 3, 16, 16]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.pre_norm.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual.pre_norm.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual_final_norm.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.visual_final_norm.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.token_embedding.weight - torch.Size([49408, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.ln_final.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/25 19:35:56 - mmengine - INFO - \n",
      "backbone.ln_final.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('../configs/clip/vit-base-p16_pt-64xb64_in1k.py').model\n",
    "cfg.type = 'CLIPClassifier'\n",
    "cfg.backbone = dict(\n",
    "    type='CLIP',\n",
    "    visual=cfg.backbone,\n",
    "    text=dict(\n",
    "        type='TextTransformer',\n",
    "        context_length=77,\n",
    "        vocab_size=49408,\n",
    "        width=512,\n",
    "        output_dims=256,\n",
    "        num_heads=8,\n",
    "        num_layers=12),\n",
    "    output_dims=512,\n",
    "    init_cfg = dict(\n",
    "        type='Pretrained', \n",
    "        checkpoint='../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth'))\n",
    "cfg.head= dict(\n",
    "    type='CLIPClsHead',\n",
    "    loss=dict(type='CLIPLoss'),\n",
    "    cal_acc=True)\n",
    "model = build_classifier(cfg)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CLIPClassifier',\n",
       " 'backbone': {'type': 'CLIP',\n",
       "  'visual': {'type': 'VisionTransformer',\n",
       "   'arch': 'b',\n",
       "   'img_size': 224,\n",
       "   'patch_size': 16,\n",
       "   'drop_rate': 0.1,\n",
       "   'init_cfg': [{'type': 'Kaiming',\n",
       "     'layer': 'Conv2d',\n",
       "     'mode': 'fan_in',\n",
       "     'nonlinearity': 'linear'}],\n",
       "   'pre_norm': True},\n",
       "  'text': {'type': 'TextTransformer',\n",
       "   'context_length': 77,\n",
       "   'vocab_size': 49408,\n",
       "   'width': 512,\n",
       "   'output_dims': 256,\n",
       "   'num_heads': 8,\n",
       "   'num_layers': 12},\n",
       "  'output_dims': 512,\n",
       "  'init_cfg': {'type': 'Pretrained',\n",
       "   'checkpoint': '../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth'}},\n",
       " 'neck': None,\n",
       " 'head': {'type': 'CLIPClsHead',\n",
       "  'loss': {'type': 'CLIPLoss'},\n",
       "  'cal_acc': True}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/hxm/ovd/mmdetection/mmdet/datasets/api_wrappers/coco_api.py:24: UserWarning: mmpycocotools is deprecated. Please install official pycocotools by \"pip install pycocotools\"\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    LoadInstanceImage(with_mask=False, exp_factor=1.2, channel_order='rgb'),\n",
    "    ResizeEdge(scale=256, edge='short'),\n",
    "    RandomCrop(crop_size=224),\n",
    "    RandomFlip(prob=0.5, direction='horizontal'),\n",
    "    PackInputs(algorithm_keys=['language'])\n",
    "]\n",
    "toy_dataset = VisionLanguageDataset(\n",
    "    InstanceDataset(\n",
    "        Objects365V2Dataset(\n",
    "            data_root='../data/Objects365/Obj365_v2/',\n",
    "            data_prefix=dict(img='train/'),\n",
    "            ann_file='debug/train.json'),\n",
    "        filter_cfg=dict(min_size=32)),\n",
    "    pipeline=pipeline)\n",
    "\n",
    "sampler = DefaultSampler(toy_dataset, shuffle=True)\n",
    "train_loader = DataLoader(dataset=toy_dataset, batch_size=4, sampler=sampler, collate_fn=pseudo_collate)\n",
    "data_preprocessor = ClsDataPreprocessor(\n",
    "    mean=[125.307, 122.961, 113.8575],\n",
    "    std=[51.5865, 50.847, 51.255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(2.5499, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.8627, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.9411, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.1341, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.3220, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.0964, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.2141, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.8033, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.0254, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.9332, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.6824, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.4030, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.9817, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.8273, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.0837, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.1177, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.9946, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.4238, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.0435, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.3783, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.3476, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.4014, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.4030, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.2969, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(1.7812, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(4.1338, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(1.7789, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.3429, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.3741, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.6982, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.5254, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.3236, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.9368, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.5274, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.1518, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.3914, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.4511, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.3345, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.5063, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.2105, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.0912, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.5305, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.7884, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.2610, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.0304, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.5173, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.0351, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.5809, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.1417, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.9213, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.6347, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.1782, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.0781, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.2113, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.6399, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.4173, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.3558, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(1.8060, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.1692, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.0651, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.4410, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.3954, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.8158, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.9636, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.6594, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.1200, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(3.8471, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.8910, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.9635, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.1953, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.6613, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.7553, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(3.7449, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.5802, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.9237, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.9288, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.2405, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.7331, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.7231, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.9048, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(4.4615, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.5326, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.7508, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.2760, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.7570, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.9401, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.7908, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.6099, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.5441, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.2520, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.8410, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.7175, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.9746, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.9189, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.5478, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.3290, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.9476, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.2444, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.3334, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.0996, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.5188, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.5881, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.4019, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.0125, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.4245, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(1.2733, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.1762, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.6408, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.1095, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.4559, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.6738, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(4.1003, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.7896, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.8937, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(0., grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([100.])]}\n"
     ]
    }
   ],
   "source": [
    "# model = model.cuda()\n",
    "# data_preprocessor = data_preprocessor.cuda()\n",
    "# 训练过程\n",
    "for data_batch in train_loader:\n",
    "#     data_batch['inputs'] = [d.cuda() for d in data_batch['inputs']]\n",
    "#     data_batch['data_samples'] = [d.cuda() for d in data_batch['data_samples']]\n",
    "\n",
    "    data_batch = data_preprocessor(data_batch, training=True)\n",
    "    if isinstance(data_batch, dict):\n",
    "        losses = model(**data_batch, mode='loss')\n",
    "    elif isinstance(data_batch, (list, tuple)):\n",
    "        losses = model(*data_batch, mode='loss')\n",
    "    else:\n",
    "        raise TypeError()\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x, *args):\n",
    "    # 对x进行处理\n",
    "    processed_x = x * 2\n",
    "\n",
    "    # 输出处理后的x\n",
    "    print(\"Processed x:\", processed_x)\n",
    "\n",
    "    # 输出原始的*args参数（展开）\n",
    "    print(\"*args:\", *args)\n",
    "\n",
    "    # 返回处理后的x和原始的*args参数\n",
    "    return processed_x, args\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet_3x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
