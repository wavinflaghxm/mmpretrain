{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmengine.dataset import DefaultSampler, pseudo_collate\n",
    "from mmdet.datasets.objects365 import Objects365V2Dataset\n",
    "from mmpretrain.datasets.transforms import *\n",
    "from mmpretrain.models import ClsDataPreprocessor\n",
    "\n",
    "from mmengine import Config\n",
    "from mmpretrain.models import build_classifier\n",
    "\n",
    "from projects.ma_clip.datasets import InstanceDataset, LoadInstanceImage\n",
    "from projects.ma_clip.models import *\n",
    "from projects.clip.datasets import *\n",
    "from projects.clip.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmpretrain.models import build_classifier\n",
    "from mmpretrain.datasets import build_dataset\n",
    "from mmengine import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29 16:47:43 - mmengine - INFO - load model from: data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth\n",
      "05/29 16:47:43 - mmengine - INFO - Loads checkpoint by local backend from path: data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth\n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual_projection - torch.Size([768, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.positional_embedding - torch.Size([77, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.text_projection - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.logit_scale - torch.Size([]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.cls_token - torch.Size([1, 1, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.pos_embed - torch.Size([1, 197, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.patch_embed.projection.weight - torch.Size([768, 3, 16, 16]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.pre_norm.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual.pre_norm.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual_final_norm.weight - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.visual_final_norm.bias - torch.Size([768]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.token_embedding.weight - torch.Size([49408, 512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.ln_final.weight - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "backbone.ln_final.bias - torch.Size([512]): \n",
      "PretrainedInit: load from data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.0.weight - torch.Size([16, 1, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.0.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.1.weight - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.1.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.3.weight - torch.Size([32, 16, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.3.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.4.weight - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.4.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.6.weight - torch.Size([64, 32, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.6.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.7.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.7.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.9.weight - torch.Size([128, 64, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.9.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.10.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.10.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.12.weight - torch.Size([768, 128, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.fore_layers.12.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.0.weight - torch.Size([16, 1, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.0.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.1.weight - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.1.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.3.weight - torch.Size([32, 16, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.3.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.4.weight - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.4.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.6.weight - torch.Size([64, 32, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.6.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.7.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.7.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.9.weight - torch.Size([128, 64, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.9.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.10.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.10.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.12.weight - torch.Size([768, 128, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.encoder.back_layers.12.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_pos_embed - torch.Size([1, 198, 1024]): \n",
      "Initialized by user-defined `init_weights` in MACLIPMaskDecoder  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_embed.weight - torch.Size([1024, 1536]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_embed.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_norm.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_norm.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_proj.weight - torch.Size([512, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/29 16:47:43 - mmengine - INFO - \n",
      "neck.decoder.decoder_proj.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('projects/ma_clip/configs/vit-base-p16_pt-64xb64_in1k.py')\n",
    "model = build_classifier(cfg.model)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/hxm/ovd/mmdetection/mmdet/datasets/api_wrappers/coco_api.py:24: UserWarning: mmpycocotools is deprecated. Please install official pycocotools by \"pip install pycocotools\"\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=48.16s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(cfg.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DefaultSampler(dataset, shuffle=True)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=4, sampler=sampler, collate_fn=pseudo_collate)\n",
    "data_preprocessor = ClsDataPreprocessor(\n",
    "    mean=[125.307, 122.961, 113.8575, 0.0],\n",
    "    std=[51.5865, 50.847, 51.255, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.9239, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(3.7734, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(1.4454, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.6709, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(1.6898, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.5910, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(3.7946, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.4330, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(2.0179, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(3.2488, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n",
      "{'loss': tensor(1.6471, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(2.7211, grad_fn=<MulBackward0>), 'accuracy_top-1': [tensor([25.])]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30378/2070748208.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/mmpretrain/models/classifiers/image.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_head\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/clip/models/clip.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, inputs, data_samples)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/clip/models/clip.py\u001b[0m in \u001b[0;36mextract_feat\u001b[0;34m(self, inputs, data_samples)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataSample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_neck\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     def loss(self, inputs: List[Tensor],\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/ma_clip/models/maclip_neck.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmask_fore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx_fore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask_fore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# b c h w -> b (h w) c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/ma_clip/models/maclip_neck.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mB\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_H\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \"\"\"\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfore_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/mmpretrain/models/utils/norm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, data_format)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias,\n\u001b[0;32m---> 86\u001b[0;31m                              self.eps)\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;31m# If the output is discontiguous, it may cause some unexpected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# problem in the downstream tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         )\n\u001b[0;32m-> 2347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = model.cuda()\n",
    "# data_preprocessor = data_preprocessor.cuda()\n",
    "# 训练过程\n",
    "for data_batch in train_loader:\n",
    "#     data_batch['inputs'] = [d.cuda() for d in data_batch['inputs']]\n",
    "#     data_batch['data_samples'] = [d.cuda() for d in data_batch['data_samples']]\n",
    "\n",
    "    data_batch = data_preprocessor(data_batch, training=True)\n",
    "    if isinstance(data_batch, dict):\n",
    "        losses = model(**data_batch, mode='loss')\n",
    "    elif isinstance(data_batch, (list, tuple)):\n",
    "        losses = model(*data_batch, mode='loss')\n",
    "    else:\n",
    "        raise TypeError()\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/hxm/ovd/mmdetection/mmdet/datasets/api_wrappers/coco_api.py:24: UserWarning: mmpycocotools is deprecated. Please install official pycocotools by \"pip install pycocotools\"\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    LoadInstanceImage(with_mask=True, exp_factor=1.2, channel_order='rgb'),\n",
    "    ResizeEdge(scale=256, edge='short'),\n",
    "    RandomCrop(crop_size=224),\n",
    "    RandomFlip(prob=0.5, direction='horizontal'),\n",
    "    PackInputs(algorithm_keys=['language'])\n",
    "]\n",
    "toy_dataset = VLDataset(\n",
    "    InstanceDataset(\n",
    "        Objects365V2Dataset(\n",
    "            data_root='../data/Objects365/Obj365_v2/',\n",
    "            data_prefix=dict(img='train/'),\n",
    "            ann_file='debug/train.json'),\n",
    "        filter_cfg=dict(min_size=32)),\n",
    "    pipeline=pipeline)\n",
    "\n",
    "sampler = DefaultSampler(toy_dataset, shuffle=True)\n",
    "train_loader = DataLoader(dataset=toy_dataset, batch_size=2, sampler=sampler, collate_fn=pseudo_collate)\n",
    "data_preprocessor = ClsDataPreprocessor(\n",
    "    mean=[125.307, 122.961, 113.8575, 0.0],\n",
    "    std=[51.5865, 50.847, 51.255, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/24 17:10:03 - mmengine - INFO - load model from: ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth\n",
      "05/24 17:10:03 - mmengine - INFO - Loads checkpoint by local backend from path: ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth\n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual_projection - torch.Size([768, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.positional_embedding - torch.Size([77, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.text_projection - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.logit_scale - torch.Size([]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.cls_token - torch.Size([1, 1, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.pos_embed - torch.Size([1, 197, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.patch_embed.projection.weight - torch.Size([768, 3, 16, 16]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.0.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.1.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.2.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.3.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.4.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.5.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.6.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.7.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.8.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.9.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.10.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln1.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.qkv.bias - torch.Size([2304]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.proj.weight - torch.Size([768, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.attn.proj.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln2.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ln2.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.0.0.bias - torch.Size([3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.1.weight - torch.Size([768, 3072]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.layers.11.ffn.layers.1.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.pre_norm.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual.pre_norm.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual_final_norm.weight - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.visual_final_norm.bias - torch.Size([768]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.0.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.1.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.2.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.3.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.4.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.5.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.6.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.7.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.8.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.9.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.10.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.in_proj_weight - torch.Size([1536, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.in_proj_bias - torch.Size([1536]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.out_proj.weight - torch.Size([512, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.attn.out_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.ln_2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_fc.weight - torch.Size([2048, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_fc.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_proj.weight - torch.Size([512, 2048]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.transformer.resblocks.11.mlp.c_proj.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.token_embedding.weight - torch.Size([49408, 512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.ln_final.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "backbone.ln_final.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.0.weight - torch.Size([16, 1, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.0.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.1.weight - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.1.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.3.weight - torch.Size([32, 16, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.3.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.4.weight - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.4.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.6.weight - torch.Size([64, 32, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.6.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.7.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.7.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.9.weight - torch.Size([128, 64, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.9.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.10.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.10.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.12.weight - torch.Size([768, 128, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.fore_downsample_layers.12.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.0.weight - torch.Size([16, 1, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.0.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.1.weight - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.1.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.3.weight - torch.Size([32, 16, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.3.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.4.weight - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.4.bias - torch.Size([32]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.6.weight - torch.Size([64, 32, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.6.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.7.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.7.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.9.weight - torch.Size([128, 64, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.9.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.10.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.10.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.12.weight - torch.Size([768, 128, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.encoder.back_downsample_layers.12.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_pos_embed - torch.Size([1, 198, 1024]): \n",
      "Initialized by user-defined `init_weights` in MACLIPMaskDecoder  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_embed.weight - torch.Size([1024, 1536]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_embed.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.0.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.1.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.2.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.3.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.4.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.5.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.6.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln1.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.qkv.weight - torch.Size([3072, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.qkv.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.proj.weight - torch.Size([1024, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.attn.proj.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln2.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ln2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.0.0.weight - torch.Size([4096, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.0.0.bias - torch.Size([4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.1.weight - torch.Size([1024, 4096]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_blocks.7.ffn.layers.1.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_norm.weight - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_norm.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_proj.weight - torch.Size([512, 1024]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n",
      "05/24 17:10:04 - mmengine - INFO - \n",
      "neck.decoder.decoder_proj.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of CLIPClassifier  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('../configs/clip/vit-base-p16_pt-64xb64_in1k.py').model\n",
    "cfg.type = 'CLIPClassifier'\n",
    "cfg.backbone = dict(\n",
    "    type='MACLIP',\n",
    "    visual_cfg=cfg.backbone,\n",
    "    text_cfg=dict(\n",
    "        type='TextTransformer',\n",
    "        context_length=77,\n",
    "        vocab_size=49408,\n",
    "        width=512,\n",
    "        num_heads=8,\n",
    "        num_layers=12),\n",
    "    output_dims=512,\n",
    "    out_type='raw',\n",
    "    init_cfg = dict(\n",
    "        type='Pretrained', \n",
    "        checkpoint='../data/pretrained/clip/CLIP-ViT-B-16-laion2B-s34B-b88K/pretrain.pth'))\n",
    "cfg.neck = dict(\n",
    "    type='MACLIPNeck',\n",
    "    encoder_cfg=dict(\n",
    "        type='MACLIPMaskEncoder',\n",
    "        embed_dims=768,\n",
    "        hidden_dims=128,\n",
    "    ),\n",
    "    decoder_cfg=dict(\n",
    "        type='MACLIPMaskDecoder',\n",
    "        num_patches=196,\n",
    "        embed_dims=768 * 2,\n",
    "        hidden_dims=1024,\n",
    "        output_dims=512,\n",
    "        num_layers=8,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4,\n",
    "    ), \n",
    ")\n",
    "cfg.head= dict(\n",
    "    type='CLIPClsHead',\n",
    "    loss=dict(type='CLIPLoss'),\n",
    "    cal_acc=True)\n",
    "model = build_classifier(cfg)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(2.7514, grad_fn=<SumBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(1.4961, grad_fn=<SumBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.4372, grad_fn=<SumBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(1.1959, grad_fn=<SumBackward0>), 'accuracy_top-1': [tensor([50.])]}\n",
      "{'loss': tensor(0.9737, grad_fn=<SumBackward0>), 'accuracy_top-1': [tensor([0.])]}\n",
      "{'loss': tensor(2.0520, grad_fn=<SumBackward0>), 'accuracy_top-1': [tensor([0.])]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45189/344351563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/mmpretrain/models/classifiers/image.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_head\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/ma_clip/clip/clip.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, inputs, data_samples)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \"\"\"\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/ma_clip/clip/clip.py\u001b[0m in \u001b[0;36mextract_feat\u001b[0;34m(self, inputs, data_samples)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataSample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_neck\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     def loss(self, inputs: torch.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/ma_clip/models/maclip_neck.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmask_fore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx_fore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask_fore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# b c h w -> b (h w) c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mnt/hxm/ovd/mmpretrain/projects/ma_clip/models/maclip_neck.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mB\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_H\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \"\"\"\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mmask_fore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfore_downsample_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mmask_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_downsample_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmdet_3x/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练过程\n",
    "for data_batch in train_loader:\n",
    "    data_batch = data_preprocessor(data_batch, training=True)\n",
    "    if isinstance(data_batch, dict):\n",
    "        losses = model(**data_batch, mode='loss')\n",
    "    elif isinstance(data_batch, (list, tuple)):\n",
    "        losses = model(*data_batch, mode='loss')\n",
    "    else:\n",
    "        raise TypeError()\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "64\n",
      "128\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "for i in range(4 - 1, -1, -1):\n",
    "    dims = 256 // 2**i\n",
    "    print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from projects.ma_clip.clip import VLDataset\n",
    "\n",
    "pipeline = [\n",
    "    LoadInstanceImage(with_mask=True, exp_factor=1.2, channel_order='rgb'),\n",
    "    ResizeEdge(scale=256, edge='short'),\n",
    "    RandomCrop(crop_size=224),\n",
    "    RandomFlip(prob=0.5, direction='horizontal'),\n",
    "    # PackInputs(algorithm_keys=('language',))\n",
    "]\n",
    "toy_dataset = VLDataset(\n",
    "    InstanceDataset(\n",
    "        Objects365V2Dataset(\n",
    "            data_root='../data/Objects365/Obj365_v2/',\n",
    "            data_prefix=dict(img='train/'),\n",
    "            ann_file='debug/train.json'),\n",
    "        filter_cfg=dict(min_size=32)),\n",
    "    pipeline=pipeline)\n",
    "\n",
    "sampler = DefaultSampler(toy_dataset, shuffle=True)\n",
    "train_loader = DataLoader(dataset=toy_dataset, batch_size=1, sampler=sampler, collate_fn=pseudo_collate)\n",
    "data_preprocessor = ClsDataPreprocessor(\n",
    "    mean=[125.307, 122.961, 113.8575, 0.0],\n",
    "    std=[51.5865, 50.847, 51.255, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_path': '../data/Objects365/Obj365_v2/train/patch16/objects365_v2_00900003.jpg',\n",
       " 'gt_label': 199,\n",
       " 'bbox': [336.0317382656,\n",
       "  325.89758300159997,\n",
       "  657.8232421376,\n",
       "  392.49353026560004],\n",
       " 'text': tensor([[49406,   320,  2103,  ...,     0,     0,     0],\n",
       "         [49406,   320,  1125,  ...,     0,     0,     0],\n",
       "         [49406,   320,  8757,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [49406,   320,  1125,  ...,     0,     0,     0],\n",
       "         [49406,   320,  1125,  ...,     0,     0,     0],\n",
       "         [49406,   320,  6325,  ...,     0,     0,     0]]),\n",
       " 'img': array([[[ 57,  64,  48,   0],\n",
       "         [ 59,  66,  48,   0],\n",
       "         [ 64,  68,  51,   0],\n",
       "         ...,\n",
       "         [ 98,  98,  98,   0],\n",
       "         [ 75,  73,  74,   0],\n",
       "         [ 77,  73,  74,   0]],\n",
       " \n",
       "        [[ 59,  63,  46,   0],\n",
       "         [ 43,  47,  30,   0],\n",
       "         [ 54,  58,  41,   0],\n",
       "         ...,\n",
       "         [ 97,  95,  96,   0],\n",
       "         [ 75,  73,  74,   0],\n",
       "         [ 81,  79,  80,   0]],\n",
       " \n",
       "        [[ 59,  63,  46,   0],\n",
       "         [ 53,  57,  40,   0],\n",
       "         [ 51,  55,  38,   0],\n",
       "         ...,\n",
       "         [101,  99, 100,   0],\n",
       "         [ 83,  81,  82,   0],\n",
       "         [ 80,  78,  79,   0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 36,  34,  35,   0],\n",
       "         [ 38,  36,  37,   0],\n",
       "         [ 40,  38,  39,   0],\n",
       "         ...,\n",
       "         [116, 111, 107,   0],\n",
       "         [116, 111, 108,   0],\n",
       "         [119, 115, 112,   0]],\n",
       " \n",
       "        [[ 43,  41,  42,   0],\n",
       "         [ 41,  41,  41,   0],\n",
       "         [ 42,  40,  41,   0],\n",
       "         ...,\n",
       "         [102,  94,  92,   0],\n",
       "         [103,  98,  95,   0],\n",
       "         [ 95,  91,  88,   0]],\n",
       " \n",
       "        [[ 41,  39,  40,   0],\n",
       "         [ 38,  38,  38,   0],\n",
       "         [ 42,  40,  41,   0],\n",
       "         ...,\n",
       "         [ 71,  63,  61,   0],\n",
       "         [ 82,  74,  72,   0],\n",
       "         [ 85,  80,  76,   0]]], dtype=uint8),\n",
       " 'img_shape': (79, 385),\n",
       " 'ori_shape': (79, 385)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet_3x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
