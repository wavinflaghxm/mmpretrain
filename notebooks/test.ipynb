{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from mmcv.transforms import BaseTransform\n",
    "from mmengine.utils import is_str\n",
    "from PIL import Image\n",
    "\n",
    "from mmpretrain.registry import TRANSFORMS\n",
    "from mmpretrain.structures import DataSample, MultiTaskDataSample\n",
    "from mmpretrain.datasets import PackInputs\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmengine.dataset import DefaultSampler, pseudo_collate, default_collate\n",
    "from mmdet.datasets.objects365 import Objects365V2Dataset\n",
    "from mmpretrain.datasets.transforms import *\n",
    "from mmpretrain.models import ClsDataPreprocessor\n",
    "\n",
    "from mmengine import Config\n",
    "from mmpretrain.models import build_classifier\n",
    "\n",
    "from projects.ma_clip.datasets import InstanceDataset, LoadInstanceImage\n",
    "from projects.ma_clip.models import *\n",
    "from projects.clip.models import *\n",
    "from projects.clip.datasets import *\n",
    "\n",
    "import math\n",
    "from numbers import Number\n",
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from mmengine.model import (BaseDataPreprocessor, ImgDataPreprocessor,\n",
    "                            stack_batch)\n",
    "\n",
    "from mmpretrain.registry import MODELS\n",
    "from mmpretrain.structures import (DataSample, MultiTaskDataSample,\n",
    "                                   batch_label_to_onehot, cat_batch_labels,\n",
    "                                   tensor_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackMultiInputs(PackInputs):\n",
    "\n",
    "    def transform(self, results: dict) -> dict:\n",
    "        \"\"\"Method to pack the input data.\"\"\"\n",
    "        packed_results = dict(inputs=[])\n",
    "\n",
    "        self.input_key = self.input_key \\\n",
    "            if isinstance(self.input_key, Sequence) else [self.input_key]\n",
    "        for input_key in self.input_key:\n",
    "            if input_key in results:\n",
    "                input_ = results[input_key]\n",
    "                packed_results['inputs'].append(self.format_input(input_))\n",
    "\n",
    "        data_sample = DataSample()\n",
    "\n",
    "        # Set default keys\n",
    "        if 'gt_label' in results:\n",
    "            data_sample.set_gt_label(results['gt_label'])\n",
    "        if 'gt_score' in results:\n",
    "            data_sample.set_gt_score(results['gt_score'])\n",
    "        if 'mask' in results:\n",
    "            data_sample.set_mask(results['mask'])\n",
    "\n",
    "        # Set custom algorithm keys\n",
    "        for key in self.algorithm_keys:\n",
    "            if key in results:\n",
    "                data_sample.set_field(results[key], key)\n",
    "\n",
    "        # Set meta keys\n",
    "        for key in self.meta_keys:\n",
    "            if key in results:\n",
    "                data_sample.set_field(results[key], key, field_type='metainfo')\n",
    "\n",
    "        packed_results['data_samples'] = data_sample\n",
    "        return packed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageDataPreprocessor(ClsDataPreprocessor):\n",
    "\n",
    "    def forward(self, data: dict, training: bool = False) -> dict:\n",
    "        \"\"\"Perform normalization, padding, bgr2rgb conversion and batch\n",
    "        augmentation based on ``BaseDataPreprocessor``.\n",
    "\n",
    "        Args:\n",
    "            data (dict): data sampled from dataloader.\n",
    "            training (bool): Whether to enable training time augmentation.\n",
    "\n",
    "        Returns:\n",
    "            dict: Data in the same format as the model input.\n",
    "        \"\"\"\n",
    "        inputs = self.cast_data(data['inputs'])\n",
    "\n",
    "        vision, language = inputs\n",
    "        if isinstance(vision, torch.Tensor):\n",
    "            # The branch if use `default_collate` as the collate_fn in the\n",
    "            # dataloader.\n",
    "\n",
    "            # ------ Vision ------\n",
    "            # ------ To RGB ------\n",
    "            if self.to_rgb and vision.size(1) == 3:\n",
    "                vision = vision.flip(1)\n",
    "\n",
    "            # --- Normalization ---\n",
    "            vision = vision.float()\n",
    "            if self._enable_normalize:\n",
    "                vision = (vision - self.mean) / self.std\n",
    "\n",
    "            # ------ Padding -----\n",
    "            if self.pad_size_divisor > 1:\n",
    "                h, w = vision.shape[-2:]\n",
    "\n",
    "                target_h = math.ceil(\n",
    "                    h / self.pad_size_divisor) * self.pad_size_divisor\n",
    "                target_w = math.ceil(\n",
    "                    w / self.pad_size_divisor) * self.pad_size_divisor\n",
    "                pad_h = target_h - h\n",
    "                pad_w = target_w - w\n",
    "                vision = F.pad(vision, (0, pad_w, 0, pad_h), 'constant',\n",
    "                               self.pad_value)\n",
    "            # ----- Language -----\n",
    "            language = language.float()\n",
    "        else:\n",
    "            # The branch if use `pseudo_collate` as the collate_fn in the\n",
    "            # dataloader.\n",
    "\n",
    "            processed_vision = []\n",
    "            processed_language = []\n",
    "            for vision_, language_ in zip(vision, language):\n",
    "                # ------ Vision ------\n",
    "                # ------ To RGB ------\n",
    "                if self.to_rgb and vision_.size(0) == 3:\n",
    "                    vision_ = vision_.flip(0)\n",
    "\n",
    "                # --- Normalization ---\n",
    "                vision_ = vision_.float()\n",
    "                if self._enable_normalize:\n",
    "                    vision_ = (vision_ - self.mean) / self.std\n",
    "\n",
    "                # ----- Language -----\n",
    "                language_ = language_.float()\n",
    "                \n",
    "                processed_vision.append(vision_)\n",
    "                processed_language.append(language_)\n",
    "            # Combine padding and stack\n",
    "            vision = stack_batch(processed_vision, self.pad_size_divisor,\n",
    "                                 self.pad_value)\n",
    "            language = stack_batch(processed_language, self.pad_size_divisor,\n",
    "                                 self.pad_value)\n",
    "\n",
    "        data_samples = data.get('data_samples', None)\n",
    "        sample_item = data_samples[0] if data_samples is not None else None\n",
    "\n",
    "        if isinstance(sample_item, DataSample):\n",
    "            batch_label = None\n",
    "            batch_score = None\n",
    "\n",
    "            if 'gt_label' in sample_item:\n",
    "                gt_labels = [sample.gt_label for sample in data_samples]\n",
    "                batch_label, label_indices = cat_batch_labels(gt_labels)\n",
    "                batch_label = batch_label.to(self.device)\n",
    "            if 'gt_score' in sample_item:\n",
    "                gt_scores = [sample.gt_score for sample in data_samples]\n",
    "                batch_score = torch.stack(gt_scores).to(self.device)\n",
    "            elif self.to_onehot and 'gt_label' in sample_item:\n",
    "                assert batch_label is not None, \\\n",
    "                    'Cannot generate onehot format labels because no labels.'\n",
    "                num_classes = self.num_classes or sample_item.get(\n",
    "                    'num_classes')\n",
    "                assert num_classes is not None, \\\n",
    "                    'Cannot generate one-hot format labels because not set ' \\\n",
    "                    '`num_classes` in `data_preprocessor`.'\n",
    "                batch_score = batch_label_to_onehot(\n",
    "                    batch_label, label_indices, num_classes).to(self.device)\n",
    "\n",
    "            # ----- Batch Augmentations ----\n",
    "            if (training and self.batch_augments is not None\n",
    "                    and batch_score is not None):\n",
    "                inputs, batch_score = self.batch_augments(inputs, batch_score)\n",
    "\n",
    "            # ----- scatter labels and scores to data samples ---\n",
    "            if batch_label is not None:\n",
    "                for sample, label in zip(\n",
    "                        data_samples, tensor_split(batch_label,\n",
    "                                                   label_indices)):\n",
    "                    sample.set_gt_label(label)\n",
    "            if batch_score is not None:\n",
    "                for sample, score in zip(data_samples, batch_score):\n",
    "                    sample.set_gt_score(score)\n",
    "        elif isinstance(sample_item, MultiTaskDataSample):\n",
    "            data_samples = self.cast_data(data_samples)\n",
    "\n",
    "        return {'inputs': [vision, language], 'data_samples': data_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/hxm/ovd/mmdetection/mmdet/datasets/api_wrappers/coco_api.py:24: UserWarning: mmpycocotools is deprecated. Please install official pycocotools by \"pip install pycocotools\"\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    LoadInstanceImage(with_mask=False, exp_factor=1.2, channel_order='rgb'),\n",
    "    ResizeEdge(scale=256, edge='short'),\n",
    "    RandomCrop(crop_size=224),\n",
    "    RandomFlip(prob=0.5, direction='horizontal'),\n",
    "    PackMultiInputs(input_key=['img', 'text'])\n",
    "]\n",
    "toy_dataset = VisionTemplateLanguageDataset(\n",
    "    InstanceDataset(\n",
    "        Objects365V2Dataset(\n",
    "            data_root='../data/Objects365/Obj365_v2/',\n",
    "            data_prefix=dict(img='train/'),\n",
    "            ann_file='debug/train.json'),\n",
    "        filter_cfg=dict(min_size=32)),\n",
    "    pipeline=pipeline)\n",
    "\n",
    "sampler = DefaultSampler(toy_dataset, shuffle=True)\n",
    "train_loader = DataLoader(dataset=toy_dataset, batch_size=4, sampler=sampler, collate_fn=default_collate)\n",
    "data_preprocessor = VisionLanguageDataPreprocessor(\n",
    "    mean=[125.307, 122.961, 113.8575],\n",
    "    std=[51.5865, 50.847, 51.255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch in train_loader:\n",
    "    data_batch = data_preprocessor(data_batch, training=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[49406.,   320.,  2103.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  8757.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  6325.,  ...,     0.,     0.,     0.]],\n",
       "\n",
       "        [[49406.,   320.,  2103.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  8757.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  6325.,  ...,     0.,     0.,     0.]],\n",
       "\n",
       "        [[49406.,   320.,  2103.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  8757.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  6325.,  ...,     0.,     0.,     0.]],\n",
       "\n",
       "        [[49406.,   320.,  2103.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  8757.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  1125.,  ...,     0.,     0.,     0.],\n",
       "         [49406.,   320.,  6325.,  ...,     0.,     0.,     0.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch['inputs'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_path = '../data/Objects365/Obj365_v2/annotations/pseudo_mask/zhiyuan_objv2_train_pmask_patch0.json'\n",
    "\n",
    "with open(ori_path, 'r') as f:\n",
    "    ori_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4707109,\n",
       " {'id': 123,\n",
       "  'iscrowd': 0,\n",
       "  'isfake': 0,\n",
       "  'area': 4003.056219153999,\n",
       "  'isreflected': 0,\n",
       "  'bbox': [570.0605468672,\n",
       "   478.04602053120004,\n",
       "   136.49035642880006,\n",
       "   29.328491212799975],\n",
       "  'image_id': 900003,\n",
       "  'category_id': 22,\n",
       "  'segmentation': {'size': [768, 1024],\n",
       "   'counts': 'Po[=5jg04M2N2N2N1O2N1O1O1O1O010O000000000000000000010O0000000000000001O0000000000000001O01O00000000000000000010O000000000O100O100000001O0001O1O1O00001O00000000000000O10000001O01O0001O0000001O0000000000000000000000000000000000000000001O0000000OO1N210O1O01GfXOLZg03hXOKXg05jXOGYg0792N8H1000N4Ho`_7'},\n",
       "  'iou': 0.9364637732505798})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ori_data['annotations']), ori_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24166/1839781827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mori_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = '../data/Objects365/Obj365_v2/annotations/pseudo_mask/zhiyuan_objv2_train_pmask_mini.json'\n",
    "\n",
    "new_data = dict()\n",
    "new_data['annotations'] = []\n",
    "img_list = []\n",
    "for x in ori_data['annotations'][:2000000]:\n",
    "    new_data['annotations'].append(x)\n",
    "    img_list.append(x['image_id'])\n",
    "\n",
    "new_data['images'] = []\n",
    "for x in ori_data['images']:\n",
    "    if x['id'] in img_list:\n",
    "        new_data['images'].append(x)\n",
    "\n",
    "new_data['categories'] = ori_data['categories']\n",
    "\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(new_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet_3x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
